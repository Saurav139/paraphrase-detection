{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12902,"status":"ok","timestamp":1691634630028,"user":{"displayName":"Dev Bhartra","userId":"06863587495078471017"},"user_tz":240},"id":"wIso-ij7Djts","outputId":"8d5e1e74-6d56-4dab-ca80-bd63da583def"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n","Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.2)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.14)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.7.1)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.4)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"]}],"source":["!pip install scikit-learn pandas tensorflow transformers"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15301,"status":"ok","timestamp":1691634645318,"user":{"displayName":"Dev Bhartra","userId":"06863587495078471017"},"user_tz":240},"id":"azEg_9C_Djtu","outputId":"bdc299dd-3112-453c-f373-62e939acc9bd"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Saurav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\Saurav\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\Saurav\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\Saurav\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"data":{"text/plain":["True"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Download required NLTK resources and import libraries\n","\n","import pandas as pd\n","from tqdm import tqdm\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Lambda\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.optimizers import Adam\n","import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW, RobertaTokenizer, RobertaForSequenceClassification, AutoTokenizer\n","from torch.utils.data import DataLoader, Dataset, TensorDataset\n","import matplotlib.pyplot as plt\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3pgUg_qZk0lU"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"ssvOGFrYfzGJ"},"source":["### Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58734,"status":"ok","timestamp":1691634716789,"user":{"displayName":"Dev Bhartra","userId":"06863587495078471017"},"user_tz":240},"id":"TH3VK90_f1-w","outputId":"f9df3b26-2912-44f5-92ec-adda5c2d09b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Logistic Regression Accuracy: 0.5119\n"]}],"source":["# Load the dataset\n","\n","train_df = pd.read_csv('/content/train.tsv', sep='\\t')\n","val_df = pd.read_csv('/content/validation.tsv', sep='\\t')\n","test_df = pd.read_csv('/content/test.tsv', sep='\\t')\n","\n","# Combine the train and the validation\n","train_df = pd.concat([train_df, val_df], ignore_index=True)\n","\n","# Preprocess the text data\n","def preprocess_text(text):\n","    tokens = nltk.word_tokenize(text)\n","    preprocessed_tokens = [token.lower() for token in tokens if token.isalpha()]\n","    return ' '.join(preprocessed_tokens)\n","\n","train_df['preprocessed_sentence1'] = train_df['sentence1'].apply(preprocess_text)\n","train_df['preprocessed_sentence2'] = train_df['sentence2'].apply(preprocess_text)\n","test_df['preprocessed_sentence1'] = test_df['sentence1'].apply(preprocess_text)\n","test_df['preprocessed_sentence2'] = test_df['sentence2'].apply(preprocess_text)\n","\n","# Create feature vectors using TF-IDF\n","vectorizer = TfidfVectorizer()\n","X_train = vectorizer.fit_transform(train_df['preprocessed_sentence1'] + ' ' + train_df['preprocessed_sentence2'])\n","X_test = vectorizer.transform(test_df['preprocessed_sentence1'] + ' ' + test_df['preprocessed_sentence2'])\n","\n","# Scale the dense feature matrices (use with_mean=False to avoid centering sparse matrices)\n","scaler = StandardScaler(with_mean=False)\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Prepare the target variable\n","y_train = train_df['label']\n","y_test = test_df['label']\n","\n","# Train a logistic regression model\n","lr_model = LogisticRegression(max_iter=5000)  # Increase the number of iterations\n","lr_model.fit(X_train_scaled, y_train)\n","\n","# Make predictions on the test set\n","y_pred = lr_model.predict(X_test_scaled)\n","\n","# Evaluate the model\n","lr_accuracy = round(accuracy_score(y_test, y_pred),4)\n","print(\"Logistic Regression Accuracy:\", lr_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"doDeie7ff39F"},"source":["### Siamese NN"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":675428,"status":"ok","timestamp":1691635392209,"user":{"displayName":"Dev Bhartra","userId":"06863587495078471017"},"user_tz":240},"id":"eTcX2TWGf756","outputId":"259b9cc1-9f01-4716-ea3f-eb2c76796112"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[18], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Apply text preprocessing to the dataset\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_sentence1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_sentence2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[0;32m     25\u001b[0m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_sentence1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n","File \u001b[1;32mc:\\Users\\Saurav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Saurav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Saurav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n","File \u001b[1;32mc:\\Users\\Saurav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n","Cell \u001b[1;32mIn[18], line 18\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     16\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[0;32m     17\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39misalpha()]\n\u001b[1;32m---> 18\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menglish\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     19\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n","File \u001b[1;32mc:\\Users\\Saurav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n","File \u001b[1;32mc:\\Users\\Saurav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m contents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    219\u001b[0m         contents\u001b[38;5;241m.\u001b[39mappend(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n","File \u001b[1;32mc:\\Users\\Saurav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding(file)\n\u001b[1;32m--> 231\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_root\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen(encoding)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n","File \u001b[1;32mc:\\Users\\Saurav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:334\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileid):\n\u001b[0;32m    333\u001b[0m     _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path, fileid)\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFileSystemPathPointer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Saurav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decorator\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args[\u001b[38;5;241m0\u001b[39m], add_py3_data(args[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m init_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\Saurav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:311\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03mCreate a new path pointer for the given absolute path.\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m:raise IOError: If the given path does not exist.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    310\u001b[0m _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_path)\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m _path)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m _path\n","File \u001b[1;32mc:\\Users\\Saurav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Configure GPU memory growth\n","physical_devices = tf.config.experimental.list_physical_devices('GPU')\n","if len(physical_devices) > 0:\n","    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","\n","# Load the dataset\n","train_df = pd.read_csv('train.tsv', sep='\\t')\n","val_df = pd.read_csv('dev.tsv', sep='\\t')\n","test_df = pd.read_csv('test.tsv', sep='\\t')\n","\n","# Combine the train and the validation\n","train_df = pd.concat([train_df, val_df], ignore_index=True)\n","\n","# Preprocess the text data\n","def preprocess_text(text):\n","    tokens = word_tokenize(text)\n","    tokens = [token.lower() for token in tokens if token.isalpha()]\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [token for token in tokens if token not in stop_words]\n","    return ' '.join(tokens)\n","\n","# Apply text preprocessing to the dataset\n","train_df['preprocessed_sentence1'] = train_df['sentence1'].apply(preprocess_text)\n","train_df['preprocessed_sentence2'] = train_df['sentence2'].apply(preprocess_text)\n","test_df['preprocessed_sentence1'] = test_df['sentence1'].apply(preprocess_text)\n","test_df['preprocessed_sentence2'] = test_df['sentence2'].apply(preprocess_text)\n","\n","# Create vocabulary and tokenizer\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(train_df['preprocessed_sentence1'] + train_df['preprocessed_sentence2'])\n","\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","# Convert sentences to sequences\n","train_seq1 = tokenizer.texts_to_sequences(train_df['preprocessed_sentence1'])\n","train_seq2 = tokenizer.texts_to_sequences(train_df['preprocessed_sentence2'])\n","test_seq1 = tokenizer.texts_to_sequences(test_df['preprocessed_sentence1'])\n","test_seq2 = tokenizer.texts_to_sequences(test_df['preprocessed_sentence2'])\n","\n","# Pad sequences\n","max_seq_length = 50\n","train_seq1 = pad_sequences(train_seq1, maxlen=max_seq_length, padding='post')\n","train_seq2 = pad_sequences(train_seq2, maxlen=max_seq_length, padding='post')\n","test_seq1 = pad_sequences(test_seq1, maxlen=max_seq_length, padding='post')\n","test_seq2 = pad_sequences(test_seq2, maxlen=max_seq_length, padding='post')\n","\n","# Prepare the target variable\n","y_train = train_df['label']\n","y_test = test_df['label']\n","\n","# Siamese neural network model\n","embedding_dim = 100\n","lstm_units = 64\n","\n","input1 = Input(shape=(max_seq_length,))\n","input2 = Input(shape=(max_seq_length,))\n","\n","# Embedding layer to convert words to dense vectors\n","embedding_layer = Embedding(vocab_size, embedding_dim)\n","\n","# LSTM layer to process sequences\n","lstm_layer = LSTM(lstm_units)\n","\n","# Process inputs through embedding and LSTM layers\n","encoded1 = lstm_layer(embedding_layer(input1))\n","encoded2 = lstm_layer(embedding_layer(input2))\n","\n","# Calculate absolute difference between encoded vectors\n","merged = Lambda(lambda x: abs(x[0] - x[1]))([encoded1, encoded2])\n","\n","# Predict the probability of paraphrase\n","preds = Dense(1, activation='sigmoid')(merged)\n","\n","# Create the Siamese model\n","siamese_model = Model(inputs=[input1, input2], outputs=preds)\n","siamese_model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n","\n","# Training\n","print(\"Training started...\")\n","siamese_model.fit([train_seq1, train_seq2], y_train, epochs=50, batch_size=64, verbose=1)\n","\n","# Testing\n","print(\"Testing started...\")\n","y_pred = siamese_model.predict([test_seq1, test_seq2])\n","\n","# Convert predicted probabilities to binary predictions\n","y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]\n","\n","# Evaluate the model\n","snn_accuracy = round(accuracy_score(y_test, y_pred),4)\n","print(\"Siamese NN Accuracy:\", snn_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"-pN7EbrxhZMY"},"source":["### DistilBERT"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EqtYZoYLhhBG","outputId":"8beb8fe4-da40-47c9-88f7-1ee78cf939fe"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Epoch 1:   0%|          | 15/3588 [00:02<11:10,  5.33it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 1:  29%|██▊       | 1027/3588 [03:28<08:26,  5.06it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 1:  50%|████▉     | 1787/3588 [05:55<06:01,  4.98it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 1:  52%|█████▏    | 1868/3588 [06:11<05:22,  5.33it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 1: 100%|█████████▉| 3573/3588 [11:39<00:02,  5.28it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 1: 100%|██████████| 3588/3588 [11:42<00:00,  5.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1 Loss: 0.5498794106291057\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2:  16%|█▋        | 591/3588 [01:53<09:20,  5.34it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 2:  20%|█▉        | 701/3588 [02:14<09:01,  5.34it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 2:  23%|██▎       | 829/3588 [02:39<08:39,  5.31it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 2:  70%|██████▉   | 2498/3588 [08:00<03:43,  4.87it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 2:  71%|███████   | 2554/3588 [08:11<03:17,  5.24it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Epoch 2:  88%|████████▊ | 3172/3588 [10:10<01:18,  5.28it/s]"]}],"source":["# Load the training and test data\n","train_df = pd.read_csv('/content/train.tsv', sep='\\t')\n","val_df = pd.read_csv('/content/validation.tsv', sep='\\t')\n","test_df = pd.read_csv('/content/test.tsv', sep='\\t')\n","\n","# Combine the train and the validation\n","train_df = pd.concat([train_df, val_df], ignore_index=True)\n","\n","# Preprocess the data\n","X_train = train_df[['sentence1', 'sentence2']]\n","y_train = train_df['label']\n","X_test = test_df[['sentence1', 'sentence2']]\n","y_test = test_df['label']\n","\n","# Define a custom dataset for PyTorch\n","class ParaphraseDataset(Dataset):\n","    def __init__(self, tokenizer, sentences1, sentences2, labels):\n","        self.sentences1 = sentences1\n","        self.sentences2 = sentences2\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        sentence1 = self.sentences1.iloc[idx]\n","        sentence2 = self.sentences2.iloc[idx]\n","        label = self.labels.iloc[idx]\n","\n","        encoding = self.tokenizer.encode_plus(\n","            sentence1,\n","            sentence2,\n","            add_special_tokens=True,\n","            return_tensors=\"pt\",\n","            padding=\"max_length\",\n","            max_length=128,\n","            truncation=True\n","        )\n","\n","        input_ids = encoding[\"input_ids\"].squeeze()\n","        attention_mask = encoding[\"attention_mask\"].squeeze()\n","\n","        return {\n","            \"input_ids\": input_ids,\n","            \"attention_mask\": attention_mask,\n","            \"labels\": label\n","        }\n","\n","# Initialize the tokenizer and model\n","distilbert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","distilbert_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n","\n","# Move the model to the GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","distilbert_model.to(device)\n","\n","# Create DataLoader objects for the training and test datasets with increased batch size\n","train_dataset = ParaphraseDataset(distilbert_tokenizer, X_train[\"sentence1\"], X_train[\"sentence2\"], y_train)\n","test_dataset = ParaphraseDataset(distilbert_tokenizer, X_test[\"sentence1\"], X_test[\"sentence2\"], y_test)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=16)\n","\n","# Fine-tuning and Training the model\n","distilbert_model.train()\n","optimizer = AdamW(distilbert_model.parameters(), lr=2e-5)\n","\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    for i, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")):\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = distilbert_model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    print(f\"Epoch {epoch + 1} Loss: {running_loss / len(train_dataloader)}\")\n","\n","\n","# Save the fine-tuned model\n","model_save_path = \"distilbert_model.pth\"\n","# from google.colab import files\n","# files.download('distilbert_model.pth')\n","torch.save(distilbert_model.state_dict(), model_save_path)\n","\n","# Load the fine-tuned model\n","distilbert_model.load_state_dict(torch.load(model_save_path))\n","distilbert_model.eval()\n","\n","# Testing the model\n","y_true = []\n","y_pred = []\n","\n","with torch.no_grad():\n","    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","\n","        outputs = distilbert_model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","\n","        y_true.extend(labels.cpu().numpy())\n","        y_pred.extend(logits.argmax(1).cpu().numpy())\n","\n","# Convert predictions from numerical to binary labels (0 or 1)\n","y_pred = [1 if pred == 1 else 0 for pred in y_pred]\n","\n","# Compute performance metrics\n","db_accuracy = round(accuracy_score(y_true, y_pred),4)\n","print(\"DistilBERT Accuracy:\", db_accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jzr2x471XuHC"},"outputs":[],"source":["from google.colab import files\n","files.download('distilbert_model.pth')"]},{"cell_type":"markdown","metadata":{"id":"1VjqMpp9a5_X"},"source":["### RoBERTa"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1ZAN1a8XDjtw"},"outputs":[],"source":["# # Load the training and test data\n","# train_df = pd.read_csv('/content/train.tsv', sep='\\t')\n","# val_df = pd.read_csv('/content/validation.tsv', sep='\\t')\n","# test_df = pd.read_csv('/content/test.tsv', sep='\\t')\n","\n","# # Combine the train and the validation\n","# train_df = pd.concat([train_df, val_df], ignore_index=True)\n","\n","# # Load pre-trained RoBERTa tokenizer and model\n","# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","# model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=1)\n","\n","# # Convert sentences to lists\n","# train_sentences1 = train_df['sentence1'].tolist()\n","# train_sentences2 = train_df['sentence2'].tolist()\n","# train_labels = train_df['label'].tolist()\n","\n","# test_sentences1 = test_df['sentence1'].tolist()\n","# test_sentences2 = test_df['sentence2'].tolist()\n","# test_labels = test_df['label'].tolist()\n","\n","# # Tokenize and encode the sentences\n","# train_encodings = tokenizer(train_sentences1, train_sentences2, truncation=True, padding=True, return_tensors='pt')\n","# test_encodings = tokenizer(test_sentences1, test_sentences2, truncation=True, padding=True, return_tensors='pt')\n","\n","# # Convert labels to tensors\n","# train_labels_tensor = torch.tensor(train_labels).float().view(-1, 1)\n","# test_labels_tensor = torch.tensor(test_labels).float().view(-1, 1)\n","\n","# # Create a TensorDataset\n","# train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels_tensor)\n","# test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels_tensor)\n","\n","# # Define batch size for DataLoader\n","# batch_size = 16\n","\n","# # Create DataLoaders for training and testing\n","# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","# test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","# gpu_info = !nvidia-smi\n","# gpu_info = '\\n'.join(gpu_info)\n","# if gpu_info.find('failed') >= 0:\n","#   print('Not connected to a GPU')\n","# else:\n","#   print(gpu_info)\n","\n","# # Define the optimizer and loss function\n","# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n","# loss_fn = torch.nn.BCEWithLogitsLoss()\n","\n","# # Fine-tune the RoBERTa model\n","# epochs = 2\n","# for epoch in range(epochs):\n","#     model.train()\n","#     for batch in tqdm(train_loader,desc=\"Training:\"):\n","#         input_ids, attention_mask, labels = batch\n","\n","#         optimizer.zero_grad()\n","#         outputs = model(input_ids, attention_mask=attention_mask)[0]\n","#         loss = loss_fn(outputs, labels)\n","#         loss.backward()\n","#         optimizer.step()\n","# model_save_path = \"roberta_model.pth\"\n","# torch.save(model.state_dict(), model_save_path)\n","\n","# # Evaluate the fine-tuned model on the test set\n","# model_load_path = \"/content/roberta_model.pth\"\n","# model.load_state_dict(torch.load(model_load_path))\n","# model.eval()\n","# test_predictions = []\n","# with torch.no_grad():\n","#     for batch in test_loader:\n","#         input_ids, attention_mask, _ = batch\n","#         outputs = model(input_ids, attention_mask=attention_mask)[0]\n","#         predictions = torch.sigmoid(outputs).cpu().numpy()\n","#         test_predictions.extend(predictions)\n","\n","# # Convert predictions to binary labels (0 or 1) based on a threshold (0.5)\n","# threshold = 0.5\n","# test_predictions = [1 if p >= threshold else 0 for p in test_predictions]\n","\n","# # Calculate accuracy\n","# test_labels = [int(label.item()) for _, _, label in test_dataset]\n","# accuracy = sum([1 for pred, true in zip(test_predictions, test_labels) if pred == true]) / len(test_labels)\n","# print(f\"Test accuracy: {round(accuracy,4)}\")"]},{"cell_type":"markdown","metadata":{"id":"s8HZHZra1hvH"},"source":["### RoBERTa With GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ytupRsvt1g-8"},"outputs":[],"source":["# Load the training and test data\n","train_df = pd.read_csv('/content/train.tsv', sep='\\t')\n","val_df = pd.read_csv('/content/validation.tsv', sep='\\t')\n","test_df = pd.read_csv('/content/test.tsv', sep='\\t')\n","\n","device = torch.device(\"cuda\")\n","\n","# Combine the train and the validation\n","train_df = pd.concat([train_df, val_df], ignore_index=True)\n","\n","# Load pre-trained RoBERTa tokenizer and model\n","roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=1)\n","\n","# Convert sentences to lists\n","train_sentences1 = train_df['sentence1'].tolist()\n","train_sentences2 = train_df['sentence2'].tolist()\n","train_labels = train_df['label'].tolist()\n","\n","test_sentences1 = test_df['sentence1'].tolist()\n","test_sentences2 = test_df['sentence2'].tolist()\n","test_labels = test_df['label'].tolist()\n","\n","# Tokenize and encode the sentences\n","train_encodings = roberta_tokenizer(train_sentences1, train_sentences2, truncation=True, padding=True, return_tensors='pt')\n","test_encodings = roberta_tokenizer(test_sentences1, test_sentences2, truncation=True, padding=True, return_tensors='pt')\n","\n","# Convert labels to tensors\n","train_labels_tensor = torch.tensor(train_labels).float().view(-1, 1)\n","test_labels_tensor = torch.tensor(test_labels).float().view(-1, 1)\n","\n","train_labels_tensor = train_labels_tensor.to(device)\n","test_labels_tensor = test_labels_tensor.to(device)\n","\n","# Create a TensorDataset\n","train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels_tensor)\n","test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels_tensor)\n","\n","# Define batch size for DataLoader\n","batch_size = 16\n","\n","# Create DataLoaders for training and testing\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)\n","\n","roberta_model = roberta_model.to(device)\n","\n","# Define the optimizer and loss function\n","optimizer = torch.optim.AdamW(roberta_model.parameters(), lr=1e-5)\n","loss_fn = torch.nn.BCEWithLogitsLoss()\n","\n","# Fine-tune the RoBERTa model\n","epochs = 2\n","for epoch in range(epochs):\n","    roberta_model.train()\n","    for batch in tqdm(train_loader, desc=\"Training:\"):\n","      input_ids, attention_mask, labels = batch\n","      input_ids = input_ids.to(device)\n","      attention_mask = attention_mask.to(device)\n","      labels = labels.to(device)\n","\n","      optimizer.zero_grad()\n","      outputs = roberta_model(input_ids, attention_mask=attention_mask)[0]\n","      loss = loss_fn(outputs, labels)\n","      loss.backward()\n","      optimizer.step()\n","\n","model_save_path = \"roberta_model.pth\"\n","torch.save(roberta_model.state_dict(), model_save_path)\n","\n","# Evaluate the fine-tuned model on the test set\n","roberta_model.load_state_dict(torch.load(model_save_path))\n","roberta_model.eval()\n","\n","test_predictions = []\n","with torch.no_grad():\n","    for batch in test_loader:\n","      input_ids, attention_mask, _ = batch\n","      input_ids = input_ids.to(device)\n","      attention_mask = attention_mask.to(device)\n","      outputs = roberta_model(input_ids, attention_mask=attention_mask)[0]\n","      predictions = torch.sigmoid(outputs).cpu().numpy()\n","      test_predictions.extend(predictions)\n","\n","# Convert predictions to binary labels (0 or 1) based on a threshold (0.5)\n","threshold = 0.5\n","test_predictions = [1 if p >= threshold else 0 for p in test_predictions]\n","\n","# Calculate accuracy\n","test_labels = [int(label.item()) for _, _, label in test_dataset]\n","rb_accuracy = sum([1 for pred, true in zip(test_predictions, test_labels) if pred == true]) / len(test_labels)\n","print(\"RoBERTa Accuracy:\", round(rb_accuracy,4))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kbd1C9sVZHu_"},"outputs":[],"source":["#from google.colab import files\n","files.download('roberta_model.pth')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n","  )\n",")"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","\n","# Load the tokenizer\n","roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","\n","# Load the model architecture\n","roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=1)\n","\n","# Load the saved model weights, ignoring missing keys\n","model_save_path = \"roberta_model.pth\"\n","state_dict = torch.load(model_save_path, map_location=torch.device('cpu'))\n","roberta_model.load_state_dict(state_dict, strict=False)\n","\n","# If you're using a GPU, move the model to the GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","roberta_model.to(device)\n","\n","# Set the model to evaluation mode\n","roberta_model.eval()\n"]},{"cell_type":"markdown","metadata":{"id":"Jvd8_i9lJrXG"},"source":["### Compare Accuracies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVrbnBj6JvaI"},"outputs":[],"source":["accuracies = [lr_accuracy, snn_accuracy, db_accuracy, rb_accuracy]\n","labels = ['Logistic Regression', 'Siamese NN', 'DistilBERT', 'RoBERTa']\n","\n","plt.bar(labels, accuracies, color=['#7FB3D5', '#9AC48A', '#F2AE72', '#D092E2'])\n","plt.xlabel('Accuracies')\n","plt.ylabel('Accuracy Values')\n","plt.title('Comparison of Accuracies')\n","plt.ylim(0, 1)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ResEIBmGkQGg"},"source":["### Testing on unseen sentences"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"YhAUq4robhHy"},"outputs":[],"source":["# List of unseen sentence pairs for paraphrase testing\n","\n","test_sentences = [\n","    (\"The dog is sitting on the windowsill.\", \"A dog is perched on the windowsill.\"), # Paraphrased\n","    (\"She played the piano gracefully.\", \"Her piano playing was filled with grace.\"), # Paraphrased\n","    (\"The conference has been postponed due to unforeseen circumstances.\", \"Due to unexpected events, the conference has been rescheduled.\"), # Paraphrased\n","    (\"He's not feeling well, so he won't be coming to the party.\", \"Because he's under the weather, he won't make it to the party.\"), # Paraphrased\n","    (\"The book was so fascinating that I couldn't put it down.\", \"The book was incredibly engaging, and I couldn't stop reading it.\"), # Paraphrased\n","    (\"The sky is blue.\", \"The grass is green.\"),  # Non-paraphrased\n","    (\"They went for a walk in the park.\", \"She took a stroll in the park.\"),  # Paraphrased\n","    (\"He loves to swim.\", \"His favorite activity is swimming.\"),  # Paraphrased\n","    (\"She's a talented artist.\", \"Her artistic skills are remarkable.\"),  # Paraphrased\n","    (\"The sun rises in the east.\", \"The sun sets in the west.\")  # Non-paraphrased\n","]"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"DYrlHNpKEiS9"},"outputs":[{"ename":"NameError","evalue":"name 'distilbert_model' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# test distilbert model on unseen sentences\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mdistilbert_model\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence_pair \u001b[38;5;129;01min\u001b[39;00m test_sentences:\n","\u001b[1;31mNameError\u001b[0m: name 'distilbert_model' is not defined"]}],"source":["# test distilbert model on unseen sentences\n","\n","distilbert_model.eval()\n","with torch.no_grad():\n","    for sentence_pair in test_sentences:\n","        sentence1, sentence2 = sentence_pair\n","        inputs = distilbert_tokenizer.encode_plus(\n","            sentence1,\n","            sentence2,\n","            add_special_tokens=True,\n","            return_tensors=\"pt\",\n","            padding=\"max_length\",\n","            max_length=128,\n","            truncation=True\n","        )\n","        input_ids = inputs[\"input_ids\"]\n","        attention_mask = inputs[\"attention_mask\"]\n","        outputs = distilbert_model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        y_pred.extend(logits.argmax(1).cpu().numpy())\n","\n","predictions = []\n","predictions = [\"Paraphrased\" if pred == 1 else \"Not Paraphrased\" for pred in y_pred]\n","\n","sentence1_list = []\n","sentence2_list = []\n","for i, sentence_pair in enumerate(test_sentences):\n","    sentence1, sentence2 = sentence_pair\n","    sentence1_list.append(sentence1)\n","    sentence2_list.append(sentence2)\n","\n","df = pd.DataFrame({\n","    'Sentence 1': sentence1_list,\n","    'Sentence 2': sentence2_list,\n","    'Prediction': predictions\n","})\n","\n","print(df)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'roberta_model' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mroberta_model\u001b[49m\n","\u001b[1;31mNameError\u001b[0m: name 'roberta_model' is not defined"]}],"source":["roberta_model=fi"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"GlpR-3V78RI4"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                          Sentence 1  \\\n","0              The dog is sitting on the windowsill.   \n","1                   She played the piano gracefully.   \n","2  The conference has been postponed due to unfor...   \n","3  He's not feeling well, so he won't be coming t...   \n","4  The book was so fascinating that I couldn't pu...   \n","5                                   The sky is blue.   \n","6                  They went for a walk in the park.   \n","7                                  He loves to swim.   \n","8                           She's a talented artist.   \n","9                         The sun rises in the east.   \n","\n","                                          Sentence 2       Prediction  \n","0                A dog is perched on the windowsill.      Paraphrased  \n","1           Her piano playing was filled with grace.      Paraphrased  \n","2  Due to unexpected events, the conference has b...      Paraphrased  \n","3  Because he's under the weather, he won't make ...      Paraphrased  \n","4  The book was incredibly engaging, and I couldn...      Paraphrased  \n","5                                The grass is green.  Not Paraphrased  \n","6                     She took a stroll in the park.      Paraphrased  \n","7                 His favorite activity is swimming.      Paraphrased  \n","8                Her artistic skills are remarkable.      Paraphrased  \n","9                          The sun sets in the west.  Not Paraphrased  \n"]}],"source":["# test roberta model on unseen sentences\n","\n","roberta_model.eval()\n","test_predictions = []\n","with torch.no_grad():\n","    for sentence_pair in test_sentences:\n","        sentence1, sentence2 = sentence_pair\n","        inputs = roberta_tokenizer(sentence1, sentence2, return_tensors=\"pt\", padding=True, truncation=True)\n","        input_ids = inputs[\"input_ids\"]\n","        attention_mask = inputs[\"attention_mask\"]\n","        outputs = roberta_model(input_ids, attention_mask=attention_mask)\n","        predictions = torch.sigmoid(outputs.logits).cpu().numpy()\n","        test_predictions.append(predictions)\n","\n","# Assuming test_predictions is a list of prediction scores\n","threshold = 0.5  # You can adjust this threshold based on your needs\n","\n","predictions = []\n","for pred in test_predictions:\n","    if pred >= threshold:\n","        predictions.append(\"Paraphrased\")\n","    else:\n","        predictions.append(\"Not Paraphrased\")\n","\n","sentence1_list = []\n","sentence2_list = []\n","for i, sentence_pair in enumerate(test_sentences):\n","    sentence1, sentence2 = sentence_pair\n","    sentence1_list.append(sentence1)\n","    sentence2_list.append(sentence2)\n","\n","df = pd.DataFrame({\n","    'Sentence 1': sentence1_list,\n","    'Sentence 2': sentence2_list,\n","    'Prediction': predictions\n","})\n","\n","print(df)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"AFmz3JxaY_mM"},"outputs":[{"ename":"AttributeError","evalue":"'SequenceClassifierOutput' object has no attribute 'last_hidden_state'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[28], line 71\u001b[0m\n\u001b[0;32m     35\u001b[0m resume \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mDeveloped a custom end to end data ETL ingestion pipeline to transfer 50 million medical images from customer\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124mservers onto a data lake residing in Google Cloud Data Lake\u001b[39m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124m• Integrated the data lake using Google Cloud BigQuery,PostgreSQL and Elasticsearch with data visualization\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124m• Researched and evaluated AWS Sagemaker & Google Cloud Vertex AI for model training and deployment using\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124mCICD improving training time by 20\u001b[39m\u001b[38;5;132;01m% a\u001b[39;00m\u001b[38;5;124mnd fixed underlying issues\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     48\u001b[0m job_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mWhat You’ll Do:\u001b[39m\n\u001b[0;32m     49\u001b[0m \n\u001b[0;32m     50\u001b[0m \u001b[38;5;124mAs a key member of the Data team, the Senior Data Engineer will have the autonomy to design, build, and deploy data pipelines using our Databricks platform\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124mFamiliarity with AWS infrastructure and building CICD pipelines\u001b[39m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124mPrior experience with backend API’s using Flask or Django\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 71\u001b[0m similarity_score \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_resume_to_job_description\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimilarity Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[1;32mIn[28], line 24\u001b[0m, in \u001b[0;36mcompare_resume_to_job_description\u001b[1;34m(resume, job_description)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     23\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m roberta_model(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[1;32m---> 24\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Use the mean of the last hidden state\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     similarity_score \u001b[38;5;241m=\u001b[39m cosine_similarity(logits\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Keyword similarity adjustment\u001b[39;00m\n","\u001b[1;31mAttributeError\u001b[0m: 'SequenceClassifierOutput' object has no attribute 'last_hidden_state'"]}],"source":["def extract_keywords(text):\n","    # Simple keyword extraction using CountVectorizer\n","    vectorizer = CountVectorizer(stop_words='english')\n","    X = vectorizer.fit_transform([text])\n","    keywords = vectorizer.get_feature_names_out()\n","    return set(keywords)\n","\n","def calculate_keyword_similarity(resume, job_description):\n","    resume_keywords = extract_keywords(resume)\n","    job_keywords = extract_keywords(job_description)\n","    common_keywords = resume_keywords.intersection(job_keywords)\n","    total_keywords = resume_keywords.union(job_keywords)\n","    return len(common_keywords) / len(total_keywords) if total_keywords else 0\n","\n","def compare_resume_to_job_description(resume, job_description):\n","    # Tokenize and encode the texts\n","    encodings = roberta_tokenizer(resume, job_description, truncation=True, padding=True, return_tensors='pt')\n","    input_ids = encodings['input_ids'].to(device)\n","    attention_mask = encodings['attention_mask'].to(device)\n","\n","    # Model inference\n","    with torch.no_grad():\n","        outputs = roberta_model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.last_hidden_state.mean(dim=1)  # Use the mean of the last hidden state\n","        similarity_score = cosine_similarity(logits.cpu().numpy())\n","\n","    # Keyword similarity adjustment\n","    keyword_similarity = calculate_keyword_similarity(resume, job_description)\n","    \n","    # Combine the two similarity scores (adjust weights as needed)\n","    combined_similarity_score = 0.7 * similarity_score + 0.3 * keyword_similarity\n","    return combined_similarity_score[0][0]\n","\n","# Example usage\n","resume = \"\"\"Developed a custom end to end data ETL ingestion pipeline to transfer 50 million medical images from customer\n","servers onto a data lake residing in Google Cloud Data Lake\n","• Integrated the data lake using Google Cloud BigQuery,PostgreSQL and Elasticsearch with data visualization\n","tools like Tableau and Power BI to generate multiple interactive dashboards\n","• Created deep learning models using TensorFlow and Keras to detect hemmorages in a set of 1 million images\n","• Led a cross-functional team of 6 to implement a POC on APIGEE as an API Gateway option and later proposed\n","APIGEE to be an effective solution to the stakeholders for hosting API products and also to monetize APIs\n","• Designed a processing job using Hadoop,Spark,Kafka and GCP Dataflow to run through about a 100 million\n","images in the datalake to make certain changes in the pipeline and automated running the job.\n","• Analyzed Google Cloud Spanner, Aurora, and MongoDB as potential databases for scalability and performance;\n","recommended and implemented Google Cloud Spanner, improving database query response time by 2 times\n","• Researched and evaluated AWS Sagemaker & Google Cloud Vertex AI for model training and deployment using\n","CICD improving training time by 20% and fixed underlying issues\"\"\"\n","job_description = \"\"\"What You’ll Do:\n","\n","As a key member of the Data team, the Senior Data Engineer will have the autonomy to design, build, and deploy data pipelines using our Databricks platform\n","Design, build, and maintain data pipelines to ingest batch and streaming data in production environments\n","Build QA and monitoring tooling on top of the pipelines to minimize bugs\n","This role offers the opportunity to tackle crucial business problems in a dynamic, fast-paced team, where your ability to deliver with minimal oversight will be highly valued\n","\n","\n","What You Bring:\n","\n","5+ years of experience working as a Data Engineer\n","Prior experience in building ELT data pipelines in the Databricks platform\n","Experience with: SQL, pySpark, Python\n","Adhere to simple, maintainable code and cut complexity whenever possible\n","\n","\n","You’ll Stand Out With:\n","\n","Prior experience working in a startup environment\n","Prior experience working in Business Intelligence or Data Analytics\n","Familiarity with AWS infrastructure and building CICD pipelines\n","Prior experience with backend API’s using Flask or Django\"\"\"\n","\n","similarity_score = compare_resume_to_job_description(resume, job_description)\n","print(f\"Similarity Score: {similarity_score:.4f}\")"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Similarity Score: 0.7104\n"]}],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","def extract_keywords(text):\n","    # Simple keyword extraction using CountVectorizer\n","    vectorizer = CountVectorizer(stop_words='english')\n","    X = vectorizer.fit_transform([text])\n","    keywords = vectorizer.get_feature_names_out()\n","    return set(keywords)\n","\n","def calculate_keyword_similarity(resume, job_description):\n","    resume_keywords = extract_keywords(resume)\n","    job_keywords = extract_keywords(job_description)\n","    common_keywords = resume_keywords.intersection(job_keywords)\n","    total_keywords = resume_keywords.union(job_keywords)\n","    return len(common_keywords) / len(total_keywords) if total_keywords else 0\n","\n","def compare_resume_to_job_description(resume, job_description):\n","    # Tokenize and encode the texts\n","    encodings = roberta_tokenizer(resume, job_description, truncation=True, padding=True, return_tensors='pt')\n","    input_ids = encodings['input_ids'].to(device)\n","    attention_mask = encodings['attention_mask'].to(device)\n","\n","    # Model inference\n","    with torch.no_grad():\n","        outputs = roberta_model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        # Since logits are not directly comparable for similarity, we'll use them to get embeddings\n","        # and then calculate similarity (for sequence classification this step might be different)\n","        # For simplicity, we'll use the mean of the logits for similarity calculation\n","        similarity_score = cosine_similarity(logits.cpu().numpy(), logits.cpu().numpy())\n","\n","    # Keyword similarity adjustment\n","    keyword_similarity = calculate_keyword_similarity(resume, job_description)\n","    \n","    # Combine the two similarity scores (adjust weights as needed)\n","    combined_similarity_score = 0.7 * similarity_score[0][0] + 0.3 * keyword_similarity\n","    return combined_similarity_score\n","\n","# Example usage\n","resume = \"\"\"Developed a custom end to end data ETL ingestion pipeline to transfer 50 million medical images from customer\n","servers onto a data lake residing in Google Cloud Data Lake\n","• Integrated the data lake using Google Cloud BigQuery,PostgreSQL and Elasticsearch with data visualization\n","tools like Tableau and Power BI to generate multiple interactive dashboards\n","• Created deep learning models using TensorFlow and Keras to detect hemmorages in a set of 1 million images\n","• Led a cross-functional team of 6 to implement a POC on APIGEE as an API Gateway option and later proposed\n","APIGEE to be an effective solution to the stakeholders for hosting API products and also to monetize APIs\n","• Designed a processing job using Hadoop,Spark,Kafka and GCP Dataflow to run through about a 100 million\n","images in the datalake to make certain changes in the pipeline and automated running the job.\n","• Analyzed Google Cloud Spanner, Aurora, and MongoDB as potential databases for scalability and performance;\n","recommended and implemented Google Cloud Spanner, improving database query response time by 2 times\n","• Researched and evaluated AWS Sagemaker & Google Cloud Vertex AI for model training and deployment using\n","CICD improving training time by 20% and fixed underlying issues\"\"\"\n","job_description = \"\"\"What You’ll Do:\n","\n","As a key member of the Data team, the Senior Data Engineer will have the autonomy to design, build, and deploy data pipelines using our Databricks platform\n","Design, build, and maintain data pipelines to ingest batch and streaming data in production environments\n","Build QA and monitoring tooling on top of the pipelines to minimize bugs\n","This role offers the opportunity to tackle crucial business problems in a dynamic, fast-paced team, where your ability to deliver with minimal oversight will be highly valued\n","\n","\n","What You Bring:\n","\n","5+ years of experience working as a Data Engineer\n","Prior experience in building ELT data pipelines in the Databricks platform\n","Experience with: SQL, pySpark, Python\n","Adhere to simple, maintainable code and cut complexity whenever possible\n","\n","\n","You’ll Stand Out With:\n","\n","Prior experience working in a startup environment\n","Prior experience working in Business Intelligence or Data Analytics\n","Familiarity with AWS infrastructure and building CICD pipelines\n","Prior experience with backend API’s using Flask or Django\"\"\"\n","\n","similarity_score = compare_resume_to_job_description(resume, job_description)\n","print(f\"Similarity Score: {similarity_score:.4f}\")"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Updated Similarity Score: 0.7046\n"]}],"source":["def extract_keywords(text):\n","    vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 2))\n","    X = vectorizer.fit_transform([text])\n","    keywords = vectorizer.get_feature_names_out()\n","    return set(keywords)\n","\n","def replace_similar_phrases(resume, job_description):\n","    job_keywords = extract_keywords(job_description)\n","    resume_keywords = extract_keywords(resume)\n","\n","    # Identify phrases in the job description that can replace those in the resume\n","    updated_resume = resume\n","    for keyword in job_keywords:\n","        if keyword in resume_keywords:\n","            # Find similar phrases in resume and replace them\n","            pattern = re.compile(r'\\b{}\\b'.format(re.escape(keyword)), re.IGNORECASE)\n","            updated_resume = pattern.sub(keyword, updated_resume)\n","\n","    return updated_resume\n","\n","def compare_resume_to_job_description(resume, job_description):\n","    encodings = roberta_tokenizer(resume, job_description, truncation=True, padding=True, return_tensors='pt')\n","    input_ids = encodings['input_ids'].to(device)\n","    attention_mask = encodings['attention_mask'].to(device)\n","\n","    with torch.no_grad():\n","        outputs = roberta_model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        similarity_score = cosine_similarity(logits.cpu().numpy(), logits.cpu().numpy())\n","\n","    keyword_similarity = calculate_keyword_similarity(resume, job_description)\n","    \n","    combined_similarity_score = 0.7 * similarity_score[0][0] + 0.3 * keyword_similarity\n","    return combined_similarity_score\n","\n","def calculate_keyword_similarity(resume, job_description):\n","    resume_keywords = extract_keywords(resume)\n","    job_keywords = extract_keywords(job_description)\n","    common_keywords = resume_keywords.intersection(job_keywords)\n","    total_keywords = resume_keywords.union(job_keywords)\n","    return len(common_keywords) / len(total_keywords) if total_keywords else 0\n","\n","# Example usage\n","resume = \"\"\"Developed a custom end to end data ETL ingestion pipeline to transfer 50 million medical images from customer\n","servers onto a data lake residing in Google Cloud Data Lake\n","• Integrated the data lake using Google Cloud BigQuery,PostgreSQL and Elasticsearch with data visualization\n","tools like Tableau and Power BI to generate multiple interactive dashboards\n","• Created deep learning models using TensorFlow and Keras to detect hemmorages in a set of 1 million images\n","• Led a cross-functional team of 6 to implement a POC on APIGEE as an API Gateway option and later proposed\n","APIGEE to be an effective solution to the stakeholders for hosting API products and also to monetize APIs\n","• Designed a processing job using Hadoop,Spark,Kafka and GCP Dataflow to run through about a 100 million\n","images in the datalake to make certain changes in the pipeline and automated running the job.\n","• Analyzed Google Cloud Spanner, Aurora, and MongoDB as potential databases for scalability and performance;\n","recommended and implemented Google Cloud Spanner, improving database query response time by 2 times\n","• Researched and evaluated AWS Sagemaker & Google Cloud Vertex AI for model training and deployment using\n","CICD improving training time by 20% and fixed underlying issues\"\"\"\n","job_description = \"\"\"What You’ll Do:\n","\n","As a key member of the Data team, the Senior Data Engineer will have the autonomy to design, build, and deploy data pipelines using our Databricks platform\n","Design, build, and maintain data pipelines to ingest batch and streaming data in production environments\n","Build QA and monitoring tooling on top of the pipelines to minimize bugs\n","This role offers the opportunity to tackle crucial business problems in a dynamic, fast-paced team, where your ability to deliver with minimal oversight will be highly valued\n","\n","\n","What You Bring:\n","\n","5+ years of experience working as a Data Engineer\n","Prior experience in building ELT data pipelines in the Databricks platform\n","Experience with: SQL, pySpark, Python\n","Adhere to simple, maintainable code and cut complexity whenever possible\n","\n","\n","You’ll Stand Out With:\n","\n","Prior experience working in a startup environment\n","Prior experience working in Business Intelligence or Data Analytics\n","Familiarity with AWS infrastructure and building CICD pipelines\n","Prior experience with backend API’s using Flask or Django\"\"\"\n","\n","# Update resume\n","updated_resume = replace_similar_phrases(resume, job_description)\n","\n","# Calculate similarity score with updated resume\n","similarity_score = compare_resume_to_job_description(updated_resume, job_description)\n","print(f\"Updated Similarity Score: {similarity_score:.4f}\")"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"name":"stdout","output_type":"stream","text":["Keywords missing from resume: {'platform design', 'dynamic', 'intelligence', 'opportunity', 'll stand', 'crucial', 'offers', 'key', 'senior data', 'batch streaming', 'tooling', 'offers opportunity', 'using databricks', 'paced team', 'opportunity tackle', 'python', 'django', 'data analytics', 'environment prior', 'minimal oversight', 'engineer autonomy', 'sql', 'll', 'deploy', 'maintainable code', 'python adhere', 'build qa', 'ability deliver', 'working startup', 'tackle', 'flask django', 'bugs role', 'experience', 'aws infrastructure', 'prior experience', 'deliver', 'build deploy', 'tooling pipelines', 'oversight', 'ability', 'problems', 'senior', 'design build', 'sql pyspark', 'cicd pipelines', 'backend', 'simple', 'building', 'pipelines', 'production', 'production environments', 'experience building', 'design', 'engineer prior', 'team ability', 'monitoring', 'experience working', 'cut', 'experience backend', 'deliver minimal', 'qa', 'problems dynamic', 'role offers', 'environments build', 'ingest', 'working', 'stand prior', 'streaming data', 'elt', 'fast', 'data team', 'business problems', 'platform experience', 'possible ll', 'business intelligence', 'fast paced', 'possible', 'bring', 'data production', 'simple maintainable', 'data engineer', 'member', 'building cicd', 'elt data', 'pipelines prior', 'minimize bugs', 'api using', 'startup environment', 'years', 'deploy data', 'adhere', 'pipelines ingest', 'build', 'analytics', 'maintain', 'build maintain', 'monitoring tooling', 'years experience', 'platform', 'environments', 'familiarity', 'team senior', 'code', 'building elt', 'bugs', 'crucial business', 'highly valued', 'paced', 'maintain data', 'prior', 'll key', 'analytics familiarity', 'minimize', 'autonomy', 'startup', 'batch', 'pipelines minimize', 'maintainable', 'pyspark', 'autonomy design', 'databricks', 'cut complexity', 'member data', 'environment', 'minimal', 'data pipelines', 'ingest batch', 'flask', 'bring years', 'business', 'adhere simple', 'infrastructure', 'intelligence data', 'valued bring', 'working business', 'streaming', 'working data', 'complexity possible', 'complexity', 'role', 'qa monitoring', 'pipelines databricks', 'valued', 'pipelines using', 'backend api', 'code cut', 'key member', 'highly', 'infrastructure building', 'dynamic fast', 'pyspark python', 'tackle crucial', 'engineer', 'familiarity aws', 'experience sql', 'oversight highly', 'using flask', 'databricks platform', 'stand'}\n","Updated Similarity Score: 0.7898\n"]}],"source":["updated_resume = update_resume_for_similarity(resume, job_description)\n","\n","# Calculate similarity score with updated resume\n","similarity_score = compare_resume_to_job_description(updated_resume, job_description)\n","print(f\"Updated Similarity Score: {similarity_score:.4f}\")\n"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Developed a custom end to end data ETL ingestion pipeline to transfer 50 million medical images from customer\n","servers onto a data lake residing in Google Cloud Data Lake\n","• Integrated the data lake using Google Cloud BigQuery,PostgreSQL and Elasticsearch with data visualization\n","tools like Tableau and Power BI to generate multiple interactive dashboards\n","• Created deep learning models using TensorFlow and Keras to detect hemmorages in a set of 1 million images\n","• Led a cross-functional team of 6 to implement a POC on APIGEE as an API Gateway option and later proposed\n","APIGEE to be an effective solution to the stakeholders for hosting API products and also to monetize APIs\n","• Designed a processing job using Hadoop,Spark,Kafka and GCP Dataflow to run through about a 100 million\n","images in the datalake to make certain changes in the pipeline and automated running the job.\n","• Analyzed Google Cloud Spanner, Aurora, and MongoDB as potential databases for scalability and performance;\n","recommended and implemented Google Cloud Spanner, improving database query response time by 2 times\n","• Researched and evaluated AWS Sagemaker & Google Cloud Vertex AI for model training and deployment using\n","CICD improving training time by 20% and fixed underlying issues platform design dynamic intelligence opportunity ll stand crucial offers key senior data batch streaming tooling offers opportunity using databricks paced team opportunity tackle python django data analytics environment prior minimal oversight engineer autonomy sql ll deploy maintainable code python adhere build qa ability deliver working startup tackle flask django bugs role experience aws infrastructure prior experience deliver build deploy tooling pipelines oversight ability problems senior design build sql pyspark cicd pipelines backend simple building pipelines production production environments experience building design engineer prior team ability monitoring experience working cut experience backend deliver minimal qa problems dynamic role offers environments build ingest working stand prior streaming data elt fast data team business problems platform experience possible ll business intelligence fast paced possible bring data production simple maintainable data engineer member building cicd elt data pipelines prior minimize bugs api using startup environment years deploy data adhere pipelines ingest build analytics maintain build maintain monitoring tooling years experience platform environments familiarity team senior code building elt bugs crucial business highly valued paced maintain data prior ll key analytics familiarity minimize autonomy startup batch pipelines minimize maintainable pyspark autonomy design databricks cut complexity member data environment minimal data pipelines ingest batch flask bring years business adhere simple infrastructure intelligence data valued bring working business streaming working data complexity possible complexity role qa monitoring pipelines databricks valued pipelines using backend api code cut key member highly infrastructure building dynamic fast pyspark python tackle crucial engineer familiarity aws experience sql oversight highly using flask databricks platform stand\n"]}],"source":["print(updated_resume)"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Updated Similarity Score: 0.7046\n"]}],"source":["def extract_keywords(text):\n","    vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 2))\n","    X = vectorizer.fit_transform([text])\n","    keywords = vectorizer.get_feature_names_out()\n","    return set(keywords)\n","\n","def replace_similar_phrases(resume, job_description):\n","    job_keywords = extract_keywords(job_description)\n","    resume_keywords = extract_keywords(resume)\n","\n","    # Identify phrases in the job description that can replace those in the resume\n","    updated_resume = resume\n","    for keyword in job_keywords:\n","        if keyword in resume_keywords:\n","            # Find similar phrases in resume and replace them\n","            pattern = re.compile(r'\\b{}\\b'.format(re.escape(keyword)), re.IGNORECASE)\n","            updated_resume = pattern.sub(keyword, updated_resume)\n","\n","    return updated_resume\n","\n","def compare_resume_to_job_description(resume, job_description):\n","    encodings = roberta_tokenizer(resume, job_description, truncation=True, padding=True, return_tensors='pt')\n","    input_ids = encodings['input_ids'].to(device)\n","    attention_mask = encodings['attention_mask'].to(device)\n","\n","    with torch.no_grad():\n","        outputs = roberta_model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        similarity_score = cosine_similarity(logits.cpu().numpy(), logits.cpu().numpy())\n","\n","    keyword_similarity = calculate_keyword_similarity(resume, job_description)\n","    \n","    combined_similarity_score = 0.7 * similarity_score[0][0] + 0.3 * keyword_similarity\n","    return combined_similarity_score\n","\n","def calculate_keyword_similarity(resume, job_description):\n","    resume_keywords = extract_keywords(resume)\n","    job_keywords = extract_keywords(job_description)\n","    common_keywords = resume_keywords.intersection(job_keywords)\n","    total_keywords = resume_keywords.union(job_keywords)\n","    return len(common_keywords) / len(total_keywords) if total_keywords else 0\n","\n","# Example usage\n","resume = \"\"\"Developed a custom end to end data ETL ingestion pipeline to transfer 50 million medical images from customer\n","servers onto a data lake residing in Google Cloud Data Lake\n","• Integrated the data lake using Google Cloud BigQuery,PostgreSQL and Elasticsearch with data visualization\n","tools like Tableau and Power BI to generate multiple interactive dashboards\n","• Created deep learning models using TensorFlow and Keras to detect hemmorages in a set of 1 million images\n","• Led a cross-functional team of 6 to implement a POC on APIGEE as an API Gateway option and later proposed\n","APIGEE to be an effective solution to the stakeholders for hosting API products and also to monetize APIs\n","• Designed a processing job using Hadoop,Spark,Kafka and GCP Dataflow to run through about a 100 million\n","images in the datalake to make certain changes in the pipeline and automated running the job.\n","• Analyzed Google Cloud Spanner, Aurora, and MongoDB as potential databases for scalability and performance;\n","recommended and implemented Google Cloud Spanner, improving database query response time by 2 times\n","• Researched and evaluated AWS Sagemaker & Google Cloud Vertex AI for model training and deployment using\n","CICD improving training time by 20% and fixed underlying issues\"\"\"\n","job_description = \"\"\"What You’ll Do:\n","\n","As a key member of the Data team, the Senior Data Engineer will have the autonomy to design, build, and deploy data pipelines using our Databricks platform\n","Design, build, and maintain data pipelines to ingest batch and streaming data in production environments\n","Build QA and monitoring tooling on top of the pipelines to minimize bugs\n","This role offers the opportunity to tackle crucial business problems in a dynamic, fast-paced team, where your ability to deliver with minimal oversight will be highly valued\n","\n","\n","What You Bring:\n","\n","5+ years of experience working as a Data Engineer\n","Prior experience in building ELT data pipelines in the Databricks platform\n","Experience with: SQL, pySpark, Python\n","Adhere to simple, maintainable code and cut complexity whenever possible\n","\n","\n","You’ll Stand Out With:\n","\n","Prior experience working in a startup environment\n","Prior experience working in Business Intelligence or Data Analytics\n","Familiarity with AWS infrastructure and building CICD pipelines\n","Prior experience with backend API’s using Flask or Django\"\"\"\n","\n","# Update resume\n","updated_resume = replace_similar_phrases(resume, job_description)\n","\n","# Calculate similarity score with updated resume\n","similarity_score = compare_resume_to_job_description(updated_resume, job_description)\n","print(f\"Updated Similarity Score: {similarity_score:.4f}\")"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Developed a custom end to end data ETL ingestion pipeline to transfer 50 million medical images from customer\n","servers onto a data lake residing in Google Cloud data Lake\n","• Integrated the data lake using Google Cloud BigQuery,PostgreSQL and Elasticsearch with data visualization\n","tools like Tableau and Power BI to generate multiple interactive dashboards\n","• Created deep learning models using TensorFlow and Keras to detect hemmorages in a set of 1 million images\n","• Led a cross-functional team of 6 to implement a POC on APIGEE as an api Gateway option and later proposed\n","APIGEE to be an effective solution to the stakeholders for hosting api products and also to monetize APIs\n","• Designed a processing job using Hadoop,Spark,Kafka and GCP Dataflow to run through about a 100 million\n","images in the datalake to make certain changes in the pipeline and automated running the job.\n","• Analyzed Google Cloud Spanner, Aurora, and MongoDB as potential databases for scalability and performance;\n","recommended and implemented Google Cloud Spanner, improving database query response time by 2 times\n","• Researched and evaluated aws Sagemaker & Google Cloud Vertex AI for model training and deployment using\n","cicd improving training time by 20% and fixed underlying issues\n"]}],"source":["print(updated_resume)"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Developed a custom end to end data ETL ingestion pipeline to transfer 50 million medical images from customer\n","servers onto a data lake residing in Google Cloud Data Lake\n","• Integrated the data lake using Google Cloud BigQuery,PostgreSQL and Elasticsearch with data visualization\n","tools like Tableau and Power BI to generate multiple interactive dashboards\n","• Created deep learning models using TensorFlow and Keras to detect hemmorages in a set of 1 million images\n","• Led a cross-functional team of 6 to implement a POC on APIGEE as an API Gateway option and later proposed\n","APIGEE to be an effective solution to the stakeholders for hosting API products and also to monetize APIs\n","• Designed a processing job using Hadoop,Spark,Kafka and GCP Dataflow to run through about a 100 million\n","images in the datalake to make certain changes in the pipeline and automated running the job.\n","• Analyzed Google Cloud Spanner, Aurora, and MongoDB as potential databases for scalability and performance;\n","recommended and implemented Google Cloud Spanner, improving database query response time by 2 times\n","• Researched and evaluated AWS Sagemaker & Google Cloud Vertex AI for model training and deployment using\n","CICD improving training time by 20% and fixed underlying issues\n"]}],"source":["print(resume)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
